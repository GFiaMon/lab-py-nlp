{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf31a39",
   "metadata": {},
   "source": [
    "# **Comprehensive NLP Lab: From Preprocessing to Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5fcd9",
   "metadata": {},
   "source": [
    "In this lab, you will explore a wide range of Natural Language Processing (NLP) techniques, from basic text preprocessing to advanced feature extraction and analysis. By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Tokenize** and preprocess text data.\n",
    "2. Remove **stop words** and **punctuation**.\n",
    "3. Apply **stemming** and **lemmatization**.\n",
    "4. Extract features using **Bag of Words (BoW)** and **TF-IDF**.\n",
    "5. Generate **n-grams** to capture contextual information.\n",
    "6. Evaluate the impact of different preprocessing techniques on text data.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bdb53e",
   "metadata": {},
   "source": [
    "## **1. Setup the Environment**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa12605",
   "metadata": {},
   "source": [
    "Before we begin, ensure you have the necessary libraries installed. Run the following cell to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd9b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk scikit-learn pandas matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3839e6cf",
   "metadata": {},
   "source": [
    "Now, import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e27aa77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a2544b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/guillermo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/guillermo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/guillermo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/guillermo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/guillermo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/guillermo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec67490",
   "metadata": {},
   "source": [
    "## **2. Text Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6877c3",
   "metadata": {},
   "source": [
    "### **Exercise 1: Tokenization and Stop Word Removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956bdaa1",
   "metadata": {},
   "source": [
    "Tokenize the following text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82fcea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'study', '!', 'It', 'involves', 'analyzing', 'and', 'understanding', 'human', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
    "# your code here\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26303ab9",
   "metadata": {},
   "source": [
    "Remove stop words and store the result in a variable called `filtered_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d09992c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14120510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', 'field', 'study', '!', 'involves', 'analyzing', 'understanding', 'human', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "# Create a new list. For each word in 'tokens', only keep it if it's NOT in the stop_words list.\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Filtered Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f0b30d",
   "metadata": {},
   "source": [
    "### **Exercise 2: Stemming and Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6949309",
   "metadata": {},
   "source": [
    "Apply stemming and lemmatization to the `filtered_tokens`. Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03dd22dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063d90e",
   "metadata": {},
   "source": [
    "Apply stemming and store the result in `stemmed_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6120c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  Natural ----\n",
      "PS: natur\n",
      "----  Language ----\n",
      "PS: languag\n",
      "----  Processing ----\n",
      "PS: process\n",
      "----  ( ----\n",
      "PS: (\n",
      "----  NLP ----\n",
      "PS: nlp\n",
      "----  ) ----\n",
      "PS: )\n",
      "----  fascinating ----\n",
      "PS: fascin\n",
      "----  field ----\n",
      "PS: field\n",
      "----  study ----\n",
      "PS: studi\n",
      "----  ! ----\n",
      "PS: !\n",
      "----  involves ----\n",
      "PS: involv\n",
      "----  analyzing ----\n",
      "PS: analyz\n",
      "----  understanding ----\n",
      "PS: understand\n",
      "----  human ----\n",
      "PS: human\n",
      "----  language ----\n",
      "PS: languag\n",
      "----  . ----\n",
      "PS: .\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "stemmed_tokens = []\n",
    "\n",
    "\n",
    "for word in filtered_tokens:\n",
    "    print(\"---- \",word,\"----\")\n",
    "    print('PS:',stemmer.stem(word))\n",
    "    stemmed_tokens.append(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c5d353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['natur', 'languag', 'process', '(', 'nlp', ')', 'fascin', 'field', 'studi', '!', 'involv', 'analyz', 'understand', 'human', 'languag', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc333a",
   "metadata": {},
   "source": [
    "Apply lemmatization and store the result in `lemmatized_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b23234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n",
    "# lemmatized_tokens = []\n",
    "\n",
    "\n",
    "# for word in filtered_tokens:\n",
    "#     print(\"---- \",word,\"----\")\n",
    "#     print('PS:',stemmer.stem(word))\n",
    "#     lemmatized_tokens.append(stemmer.stem(word))\n",
    "\n",
    "# Apply lemmatization to each word in the filtered_tokens list\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08f49cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fascinating', 'field', 'study', '!', 'involves', 'analyzing', 'understanding', 'human', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20531b9b",
   "metadata": {},
   "source": [
    "## **3. Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbd840",
   "metadata": {},
   "source": [
    "### **Exercise 3: Bag of Words (BoW)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7765074",
   "metadata": {},
   "source": [
    "Use the `CountVectorizer` from `scikit-learn` to create a Bag of Words representation of the following corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39c86f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I love NLP.\",\n",
    "    \"NLP is amazing.\",\n",
    "    \"I enjoy learning new things in NLP.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45334917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Step 1: Initialize the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer (min_df = 1)#(min_df=0.1,max_df=0.7)\n",
    "\n",
    "\n",
    "# Step 2: Fit and transform the corpus into a BoW representation\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfb3b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words:\n",
      " [[0 0 0 0 0 1 0 1 0]\n",
      " [1 0 0 1 0 0 0 1 0]\n",
      " [0 1 1 0 1 0 1 1 1]]\n",
      "Vocabulary: ['amazing' 'enjoy' 'in' 'is' 'learning' 'love' 'new' 'nlp' 'things']\n"
     ]
    }
   ],
   "source": [
    "print(\"Bag of Words:\\n\", X.toarray())\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d79220",
   "metadata": {},
   "source": [
    "### **Exercise 4: TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55622108",
   "metadata": {},
   "source": [
    "Use the `TfidfVectorizer` from `scikit-learn` to create a TF-IDF representation of the same corpus. Store the result in `X_tfidf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba20ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Step 1: Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df = 1)\n",
    "\n",
    "\n",
    "# Step 2: Fit and transform the corpus into a TF-IDF representation\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0df3ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      " [[0.         0.         0.         0.         0.         0.861037\n",
      "  0.         0.50854232 0.        ]\n",
      " [0.65249088 0.         0.         0.65249088 0.         0.\n",
      "  0.         0.38537163 0.        ]\n",
      " [0.         0.43238509 0.43238509 0.         0.43238509 0.\n",
      "  0.43238509 0.2553736  0.43238509]]\n",
      "Vocabulary: ['amazing' 'enjoy' 'in' 'is' 'learning' 'love' 'new' 'nlp' 'things']\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF:\\n\", X_tfidf.toarray())\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6cee3",
   "metadata": {},
   "source": [
    "### **Exercise 5: N-grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9d66d",
   "metadata": {},
   "source": [
    "Generate `bigrams (2-grams)` from the corpus using `CountVectorizer`. Store the result in `X_bigram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9de7b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Step 1: Initialize the CountVectorizer with ngram_range=(2, 2)\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Step 2: Fit and transform the corpus into a bigram representation\n",
    "\n",
    "X_bigram = bigram_vectorizer.fit_transform(corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b210fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams:\n",
      " [[0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 1 0]\n",
      " [1 1 0 1 0 1 0 1]]\n",
      "Bigram Vocabulary: ['enjoy learning' 'in nlp' 'is amazing' 'learning new' 'love nlp'\n",
      " 'new things' 'nlp is' 'things in']\n"
     ]
    }
   ],
   "source": [
    "print(\"Bigrams:\\n\", X_bigram.toarray())\n",
    "print(\"Bigram Vocabulary:\", bigram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7896acb",
   "metadata": {},
   "source": [
    "## **4. Advanced Exercise: Custom Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68072a1e",
   "metadata": {},
   "source": [
    "### **Exercise 6: Build a Custom Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb33268",
   "metadata": {},
   "source": [
    "Combine all the preprocessing steps (tokenization, stop word removal, punctuation removal, stemming/lemmatization) into a single function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfff29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def text_preprocessing_pipeline(text):\n",
    "    # Step 1: Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # print(tokens)\n",
    "\n",
    "    # Step 2: Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Step 3: Remove punctuation\n",
    "    tokens_no_punct = [word for word in filtered_tokens if word.isalpha()]\n",
    "\n",
    "\n",
    "    # (More exact way but more complex):\n",
    "    # We will check if each token is NOT a punctuation character\n",
    "    # translator = str.maketrans('', '', string.punctuation)\n",
    "    # no_punct_tokens = [word.translate(translator) for word in filtered_tokens]\n",
    "    # # Remove any empty strings that might be left after removing punctuation\n",
    "    # no_punct_tokens = [word for word in no_punct_tokens if word]\n",
    "\n",
    "    # Step 4: Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Apply lemmatization to each word in the filtered_tokens list\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_no_punct]\n",
    "\n",
    "    return lemmatized_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8170a6c",
   "metadata": {},
   "source": [
    "Apply this function to the following text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e1777799",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
    "\n",
    "# your code here\n",
    "\n",
    "processed_text = text_preprocessing_pipeline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c75b50c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text: ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'study', 'involves', 'analyzing', 'understanding', 'human', 'language']\n"
     ]
    }
   ],
   "source": [
    "print(\"Processed Text:\", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625df20",
   "metadata": {},
   "source": [
    "## **5. Evaluation of Preprocessing Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a666da0",
   "metadata": {},
   "source": [
    "### **Exercise 7: Compare Preprocessing Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae05a37",
   "metadata": {},
   "source": [
    "Compare the results of stemming and lemmatization on the following sentence. Store the results in `stemmed_tokens` and `lemmatized_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e70cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The cats are playing with the mice in the garden.\"\n",
    "# your code here\n",
    "# Step 1: Tokenize and preprocess the sentence and store the result in filtered_tokens\n",
    "\n",
    "# A) Tokenize\n",
    "tokens = word_tokenize(sentence)\n",
    "# B) Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "# C) Remove punctuation (using the same method as before)\n",
    "tokens_no_punct = [word for word in filtered_tokens if word.isalpha()]\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Apply stemming\n",
    "stemmer = PorterStemmer()\n",
    "# Apply stemming to each word in the filtered_tokens list\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens_no_punct]\n",
    "\n",
    "\n",
    "# Step 3: Apply lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Apply lemmatization to each word in the filtered_tokens list\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_no_punct]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "125ca6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens: ['cats', 'playing', 'mice', 'garden', '.']\n",
      "Stemmed Tokens: ['cat', 'play', 'mice', 'garden']\n",
      "Lemmatized Tokens: ['cat', 'playing', 'mouse', 'garden']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Tokens:\", filtered_tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c465f",
   "metadata": {},
   "source": [
    "## **6. Real-World Dataset: Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a598d9",
   "metadata": {},
   "source": [
    "### **Exercise 8: Preprocess and Analyze Tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056ab80",
   "metadata": {},
   "source": [
    "In this exercise, you will work with a real-world dataset of tweets. The dataset contains 5000 positive and 5000 negative tweets. Your task is to preprocess the tweets and extract features for sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1e37f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/guillermo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2c60819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43847ae",
   "metadata": {},
   "source": [
    "Load the dataset of positive and negative tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b423ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b8248",
   "metadata": {},
   "source": [
    "Combine them into a single list called ``all_tweets`` and create a corresponding list of labels called `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "271a4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Combine the datasets\n",
    "all_tweets = positive_tweets + negative_tweets\n",
    "\n",
    "# Create labels: 1 for positive, 0 for negative\n",
    "# The first len(positive_tweets) are positive, the rest are negative\n",
    "labels = [1] * len(positive_tweets) + [0] * len(negative_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32ec3ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Tweet: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Print a sample tweet\n",
    "print(\"Sample Tweet:\", all_tweets[0])\n",
    "print(\"Label:\", labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2a04a",
   "metadata": {},
   "source": [
    "### **Exercise 9: Preprocess Tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1e984",
   "metadata": {},
   "source": [
    "Apply the custom preprocessing pipeline to the entire dataset of tweets. Store the result in ``preprocessed_tweets``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39e8c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply the preprocessing pipeline to all tweets\n",
    "# your code here\n",
    "\n",
    "# Apply the function to every tweet in the all_tweets list\n",
    "preprocessed_tweets = [text_preprocessing_pipeline(tweet) for tweet in all_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "edeef254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Tweets Sample: ['FollowFriday', 'top', 'engaged', 'member', 'community', 'week']\n"
     ]
    }
   ],
   "source": [
    "# Print a sample preprocessed tweet\n",
    "print(\"Preprocessed Tweets Sample:\", preprocessed_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8658daf",
   "metadata": {},
   "source": [
    "### **Exercise 10: Feature Extraction on Tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f161c",
   "metadata": {},
   "source": [
    "Extract features from the preprocessed tweets using **Bag of Words** and **TF-IDF**. Store the results in ``X_bow`` and ``X_tfidf``, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bee42f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Step 1: Create a Bag of Words representation\n",
    "# First, convert each list of tokens back into a string\n",
    "corpus_strings = [' '.join(tweet) for tweet in preprocessed_tweets]\n",
    "\n",
    "# Now, initialize the vectorizer and fit it\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_bow = bow_vectorizer.fit_transform(corpus_strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a564bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a TF-IDF representation\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912687c",
   "metadata": {},
   "source": [
    "## **7. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ed4ac",
   "metadata": {},
   "source": [
    "In this lab, you explored a wide range of NLP techniques, from basic text preprocessing to advanced feature extraction and analysis. You also worked with a real-world dataset of tweets and applied your knowledge to preprocess and extract features for sentiment analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "743e848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 75.05%\n"
     ]
    }
   ],
   "source": [
    "# --- THE NEXT STEP: TRAINING A MODEL ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Choose your features (X) and labels (y)\n",
    "# Let's use the TF-IDF features we made, as they are often better\n",
    "X = X_tfidf # Our TF-IDF numbers (the features)\n",
    "y = labels   # Our list of 1s and 0s (the target we want to predict)\n",
    "\n",
    "# 2. Split the data into Training and Testing sets\n",
    "# We train the model on one part and test its accuracy on a part it has never seen.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Choose and train a Machine Learning model\n",
    "# We'll use Logistic Regression, which is great for yes/no (positive/negative) classification.\n",
    "model = LogisticRegression(max_iter=1000) # max_iter is just to make sure it finishes calculating\n",
    "model.fit(X_train, y_train) # This is the line that \"trains\" the model!\n",
    "\n",
    "# 4. Use the model to make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 5. See how accurate our model is!\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2%}\") # This will print a percentage like \"Model Accuracy: 75.50%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4880ef27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy on Test Set: 75.05%\n",
      "\n",
      "Sample Predictions from the Test Set:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Original Tweet",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Cleaned Words",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "True Sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Predicted Sentiment",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "04677bbc-dcb7-4e3d-b1bb-f740b7504957",
       "rows": [
        [
         "0",
         "I love you, how but you? @Taecyeon2pm8 did you feel the same? Emm I think not :(",
         "love feel Emm think",
         "Negative",
         "Negative"
        ],
        [
         "1",
         "@mayusushita @dildeewana_ @sonalp2591 @deepti_ahmd @armansushita8 Thanks Guys :)",
         "mayusushita Thanks Guys",
         "Positive",
         "Positive"
        ],
        [
         "2",
         "Your love, O Lord, is better than life. :) &lt;3 https://t.co/KPCeYJqKLM",
         "love Lord better life lt http",
         "Positive",
         "Positive"
        ],
        [
         "3",
         "@yasminyasir96 yeah but it will be better if we use her official Account :) Like The Other @PracchiNDesai ❤️",
         "yeah better use official Account Like PracchiNDesai",
         "Positive",
         "Positive"
        ],
        [
         "4",
         "Ok good night I wish troye wasn't ugly and I met him today:)():)!:!; but ok today was fun I'm excited for tmrw!!",
         "Ok good night wish troye ugly met today ok today fun excited tmrw",
         "Positive",
         "Positive"
        ],
        [
         "5",
         "@scottybev I'm not surprised, that sounds hellish! Why would you do such a thing? :(",
         "scottybev surprised sound hellish would thing",
         "Negative",
         "Positive"
        ],
        [
         "6",
         "Dry, hot, scorching summer #FF :) @infocffm @MediationMK @ExeterMediation @KentFMS @EssexMediation",
         "Dry hot scorching summer FF infocffm MediationMK ExeterMediation KentFMS EssexMediation",
         "Positive",
         "Positive"
        ],
        [
         "7",
         "@hanbined sad pray for me :(((",
         "hanbined sad pray",
         "Negative",
         "Negative"
        ],
        [
         "8",
         "Popol day too :(",
         "Popol day",
         "Negative",
         "Positive"
        ],
        [
         "9",
         "My Song of the Week is Ducktails - Surreal Exposure #SOTW https://t.co/BeXVWh7zIR Jingly jangly loveliness! :-)",
         "Song Week Ducktails Surreal Exposure SOTW http Jingly jangly loveliness",
         "Positive",
         "Positive"
        ],
        [
         "10",
         "I have no interest in speaking to you ever again. :-)",
         "interest speaking ever",
         "Positive",
         "Negative"
        ],
        [
         "11",
         "@caylahhhh lmfao seriously??? I can't remember if I did honestly..more likely tho :((",
         "caylahhhh lmfao seriously ca remember honestly likely tho",
         "Negative",
         "Negative"
        ],
        [
         "12",
         "Should have taken a pic before mrs wong confiscated my art work :(",
         "taken pic mr wong confiscated art work",
         "Negative",
         "Negative"
        ],
        [
         "13",
         "@Shrimp_89 @KianEganWL Aarww...I wish I could be there too :( xx",
         "KianEganWL Aarww wish could xx",
         "Negative",
         "Negative"
        ],
        [
         "14",
         "@theprincesszooz But I see what you're going at. :) Yes. Subjective pain may not be real, but that does not make it less painful.",
         "theprincesszooz see going Yes Subjective pain may real make less painful",
         "Positive",
         "Positive"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 15
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Tweet</th>\n",
       "      <th>Cleaned Words</th>\n",
       "      <th>True Sentiment</th>\n",
       "      <th>Predicted Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love you, how but you? @Taecyeon2pm8 did you feel the same? Emm I think not :(</td>\n",
       "      <td>love feel Emm think</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@mayusushita @dildeewana_ @sonalp2591 @deepti_ahmd @armansushita8 Thanks Guys :)</td>\n",
       "      <td>mayusushita Thanks Guys</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Your love, O Lord, is better than life. :) &amp;lt;3 https://t.co/KPCeYJqKLM</td>\n",
       "      <td>love Lord better life lt http</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@yasminyasir96 yeah but it will be better if we use her official Account :) Like The Other @PracchiNDesai ❤️</td>\n",
       "      <td>yeah better use official Account Like PracchiNDesai</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ok good night I wish troye wasn't ugly and I met him today:)():)!:!; but ok today was fun I'm excited for tmrw!!</td>\n",
       "      <td>Ok good night wish troye ugly met today ok today fun excited tmrw</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@scottybev I'm not surprised, that sounds hellish! Why would you do such a thing? :(</td>\n",
       "      <td>scottybev surprised sound hellish would thing</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dry, hot, scorching summer #FF :) @infocffm @MediationMK @ExeterMediation @KentFMS @EssexMediation</td>\n",
       "      <td>Dry hot scorching summer FF infocffm MediationMK ExeterMediation KentFMS EssexMediation</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@hanbined sad pray for me :(((</td>\n",
       "      <td>hanbined sad pray</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Popol day too :(</td>\n",
       "      <td>Popol day</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>My Song of the Week is Ducktails - Surreal Exposure #SOTW https://t.co/BeXVWh7zIR Jingly jangly loveliness! :-)</td>\n",
       "      <td>Song Week Ducktails Surreal Exposure SOTW http Jingly jangly loveliness</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I have no interest in speaking to you ever again. :-)</td>\n",
       "      <td>interest speaking ever</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@caylahhhh lmfao seriously??? I can't remember if I did honestly..more likely tho :((</td>\n",
       "      <td>caylahhhh lmfao seriously ca remember honestly likely tho</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Should have taken a pic before mrs wong confiscated my art work :(</td>\n",
       "      <td>taken pic mr wong confiscated art work</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@Shrimp_89 @KianEganWL Aarww...I wish I could be there too :( xx</td>\n",
       "      <td>KianEganWL Aarww wish could xx</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@theprincesszooz But I see what you're going at. :) Yes. Subjective pain may not be real, but that does not make it less painful.</td>\n",
       "      <td>theprincesszooz see going Yes Subjective pain may real make less painful</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       Original Tweet  \\\n",
       "0                                                    I love you, how but you? @Taecyeon2pm8 did you feel the same? Emm I think not :(   \n",
       "1                                                    @mayusushita @dildeewana_ @sonalp2591 @deepti_ahmd @armansushita8 Thanks Guys :)   \n",
       "2                                                            Your love, O Lord, is better than life. :) &lt;3 https://t.co/KPCeYJqKLM   \n",
       "3                        @yasminyasir96 yeah but it will be better if we use her official Account :) Like The Other @PracchiNDesai ❤️   \n",
       "4                    Ok good night I wish troye wasn't ugly and I met him today:)():)!:!; but ok today was fun I'm excited for tmrw!!   \n",
       "5                                                @scottybev I'm not surprised, that sounds hellish! Why would you do such a thing? :(   \n",
       "6                                  Dry, hot, scorching summer #FF :) @infocffm @MediationMK @ExeterMediation @KentFMS @EssexMediation   \n",
       "7                                                                                                      @hanbined sad pray for me :(((   \n",
       "8                                                                                                                    Popol day too :(   \n",
       "9                     My Song of the Week is Ducktails - Surreal Exposure #SOTW https://t.co/BeXVWh7zIR Jingly jangly loveliness! :-)   \n",
       "10                                                                              I have no interest in speaking to you ever again. :-)   \n",
       "11                                              @caylahhhh lmfao seriously??? I can't remember if I did honestly..more likely tho :((   \n",
       "12                                                                 Should have taken a pic before mrs wong confiscated my art work :(   \n",
       "13                                                                   @Shrimp_89 @KianEganWL Aarww...I wish I could be there too :( xx   \n",
       "14  @theprincesszooz But I see what you're going at. :) Yes. Subjective pain may not be real, but that does not make it less painful.   \n",
       "\n",
       "                                                                              Cleaned Words  \\\n",
       "0                                                                       love feel Emm think   \n",
       "1                                                                   mayusushita Thanks Guys   \n",
       "2                                                             love Lord better life lt http   \n",
       "3                                       yeah better use official Account Like PracchiNDesai   \n",
       "4                         Ok good night wish troye ugly met today ok today fun excited tmrw   \n",
       "5                                             scottybev surprised sound hellish would thing   \n",
       "6   Dry hot scorching summer FF infocffm MediationMK ExeterMediation KentFMS EssexMediation   \n",
       "7                                                                         hanbined sad pray   \n",
       "8                                                                                 Popol day   \n",
       "9                   Song Week Ducktails Surreal Exposure SOTW http Jingly jangly loveliness   \n",
       "10                                                                   interest speaking ever   \n",
       "11                                caylahhhh lmfao seriously ca remember honestly likely tho   \n",
       "12                                                   taken pic mr wong confiscated art work   \n",
       "13                                                           KianEganWL Aarww wish could xx   \n",
       "14                 theprincesszooz see going Yes Subjective pain may real make less painful   \n",
       "\n",
       "   True Sentiment Predicted Sentiment  \n",
       "0        Negative            Negative  \n",
       "1        Positive            Positive  \n",
       "2        Positive            Positive  \n",
       "3        Positive            Positive  \n",
       "4        Positive            Positive  \n",
       "5        Negative            Positive  \n",
       "6        Positive            Positive  \n",
       "7        Negative            Negative  \n",
       "8        Negative            Positive  \n",
       "9        Positive            Positive  \n",
       "10       Positive            Negative  \n",
       "11       Negative            Negative  \n",
       "12       Negative            Negative  \n",
       "13       Negative            Negative  \n",
       "14       Positive            Positive  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- INSPECTING THE PREDICTIONS ---\n",
    "\n",
    "# 1. First, we need to get the original text of the test tweets.\n",
    "# Remember, we split our 'corpus_strings' and 'labels' into train and test sets.\n",
    "# The X_test matrix has the same order as the test portion of 'corpus_strings' and 'all_tweets'.\n",
    "# We need to get the indices of the test set to find the original tweets.\n",
    "\n",
    "# Get the indices that were chosen for the test set\n",
    "_, _, _, _, indices_train, indices_test = train_test_split(X, y, range(len(y)), test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Now, let's create a DataFrame to view everything neatly\n",
    "import pandas as pd\n",
    "\n",
    "# Create a list of results for the test set\n",
    "results = []\n",
    "for i, index_in_original_data in enumerate(indices_test):\n",
    "    original_tweet = all_tweets[index_in_original_data]\n",
    "    cleaned_tweet = corpus_strings[index_in_original_data] # The preprocessed version we used\n",
    "    true_sentiment = \"Positive\" if y_test[i] == 1 else \"Negative\"\n",
    "    predicted_sentiment = \"Positive\" if y_pred[i] == 1 else \"Negative\"\n",
    "    results.append({\n",
    "        \"Original Tweet\": original_tweet,\n",
    "        \"Cleaned Words\": cleaned_tweet,\n",
    "        \"True Sentiment\": true_sentiment,\n",
    "        \"Predicted Sentiment\": predicted_sentiment\n",
    "    })\n",
    "\n",
    "# Convert the list to a Pandas DataFrame for a nice table view\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 3. Print the overall accuracy again and show the first 15 test samples\n",
    "print(f\"\\nModel Accuracy on Test Set: {accuracy:.2%}\\n\")\n",
    "print(\"Sample Predictions from the Test Set:\")\n",
    "pd.set_option('display.max_colwidth', None) # This ensures we can see the full tweet text\n",
    "display(results_df.head(15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasci_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
